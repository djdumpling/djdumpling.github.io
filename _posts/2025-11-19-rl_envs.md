---
title: "end-to-end: environments-to-evals (in progress)"
date: 2025-11-15
image: /public/fruit_box_updated.png
---

How well can LLMs and RL agents handle spatial reasoning in puzzle games? We develop a complete RL pipeline by developing an environment for [fruit box](https://en.gamesaien.com/game/fruit_box/) (a grid-based reasoning game) using Prime Intellect's verifiers library, benchmarking LLMs, and training RL agents to play. Repo [here](https://github.com/djdumpling/fruit_box).

This blog is roughly structured chronologically. First, writing scripted policies to generate gameplay data and validate the core environment mechanics. Second, developing the environment and integrating with Prime Intellect's [verifiers library](https://github.com/PrimeIntellect-ai/verifiers) to create an LLM-facing eval framework. Third, benchmarking said LLMs. Finally, training RL agents (SFT, GRPO, and RFT) to learn optimal play strategies.

The environment was recently merged into the [prime-environments repo](https://github.com/PrimeIntellect-ai/prime-environments/pull/293), thanks [Christian](https://x.com/creet_z) and [Sinatras](https://x.com/myainotez)!

## fruit box and intuition

[Fruit Box](https://en.gamesaien.com/game/fruit_box/) is a grid-based puzzle game played on a 10×17 board where each cell contains a digit from 1-9. The objective is to clear as many cells as possible by selecting rectangular regions that sum to exactly 10. When a valid rectangle is chosen, those cells are cleared and the player earns points equal to the number of non-zero cells cleared in that move.

**Why**? The game requires spatial reasoning, combinatorial search, and strategic planning, things that LLMs and RL agents may sometimes fail at. Unlike many benchmark environments, Fruit Box has a large discrete action space ($\binom{10+1}{2}\cdot\binom{17+1}{2}=8415$), making it a good testbed for evaluating different learning paradigms. The game also has clear optimal strategies (as we'll see), allowing us to compare learned policies against known baselines. Additionally, the multi-turn nature and the need to reason about future moves make it particularly well-suited for studying how LLMs and RL agents handle constrained sequential decision-making.

![Fruit box](/public/fruit_box_updated.png)

The game is quite fun, admittedly addictive. I didn't think higher scores were possible without faster scanning, but simple games often have 'cool tech'. One example that comes to mind is the 2048 game, where speedrun strategies always put the biggest tile in a corner (e.g. bottom-right) then button-mash arrows (e.g. down and right) to quickly accumulate large tiles. After playing several rounds, I developed key intuitions that motivate the scripted policies:

- **pairings**: 1's must go with 9's. Therefore, 1's should almost always be saved for a neighboring 9. Similarly, 2's *usually* go with 8's; in some occasions, it makes sense to select 1—1—8 over 2—8.
- **greediness**: A corollary of #1 is that the greedy algorithm is bad. It will try to find a rectangle using as many digits as possible, which increases the likelihood of using 1's or 2's. In fact, we find that the opposite of greedy (a minimal approach) is *very* effective.
- **low-level tech**: like patterns that are clearable only a certain way. For example, clearing the middle 9—1 instead of the first-column 1—9 because in the later case, the leftover 1 and 9 can't be cleared.
<div style="text-align: center;">
<img src="/public/fruit_box_tech.png" alt="Fruit box" style="max-width: 100px; height: auto;">
</div>

- **time-saving**: I find it helpful to search for the next rectangle while clearing the current one. This probably saves ~0.2 seconds per rectangle, but across the ~50 rectangles cleared, this saves ~10 seconds.
- **non-systemic**: this is person-specific, but systematic search (like starting in the top-left and working row-by-row while checking columns) is algorithmically faster due to less redundancy with overlapping cells, but it helped just to let my eyes naturally wander, clearing nearby rectangles.

## scripted policies

With this in mind, there are 4 policies that are of interest:

* **random**: choose a legal rectangle randomly. This serves as a sort of "baseline."
* **greedy**: choose the legal rectangle that maximizes cells cleared.
* **2-step lookahead**: choose the legal rectangle that maximizes a $Q$ value with discount factor.
* **minimal**: choose the legal rectangle that minimizes cells cleared.

There's a nuance to grid generation: each cell is distributed multinomially across the 9 digits, but the total sum across all 170 cells must be a multiple of 10. This is to allow a [perfect clearing](https://www.youtube.com/watch?v=Zja_MsGDKSI), though many grid configurations may be  impossible to perfect, e.g. if the number of 9's is greater  than the number of 1's. To generate valid grids, we  continuously sample sequences (10 expected) until the sum  is a multiple of 10. 

I generated 1000 seeds and benched the four policies:

![Policy Comparisons](/public/policy_comparisons.png)

<div style="text-align: center; margin-bottom: 20px;">

<table style="margin: 0 auto; width: auto; max-width: 400px;">
<thead>
<tr>
<th style="width: 150px; text-align: left;">Policy</th>
<th style="width: 80px; text-align: center;">Mean</th>
<th style="width: 80px; text-align: center;">Std</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">greedy</td>
<td style="text-align: center;">97.61</td>
<td style="text-align: center;">10.53</td>
</tr>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: center;">102.89</td>
<td style="text-align: center;">12.04</td>
</tr>
<tr>
<td style="text-align: left;">2-step lookahead</td>
<td style="text-align: center;">96.22</td>
<td style="text-align: center;">10.05</td>
</tr>
<tr>
<td style="text-align: left;">minimal</td>
<td style="text-align: center;">113.72</td>
<td style="text-align: center;">14.89</td>
</tr>
</tbody>
</table>

</div>

All four policies had roughly the same minimum score, but the minimal area policy had 5 runs which scores over 150, achieving a maximum of 157. We note a few insights:

1. **minimal beats greedy**: the minimal policy (mean 113.72) significantly outperforms greedy (mean 97.61), confirming our intuition that preserving 1's and 2's for later use is crucial. But can we teach an agent to learn this strategy? This isn't immediately clear—RL tends to work greedily, maximizing immediate reward. One initial idea is instead of a discount factor ($<1$), to use some sort of "augment factor" ($>1$) to delay actions with larger rewards, but this is likely very unstable.

2. **lookahead provides minimal benefit**: the 2-step lookahead policy (mean 96.22) performs slightly worse than greedy. This is probably because short-term lookahead isn't sufficient to capture the strategic value of preserving small numbers. This is somewhat surprising but makes sense given the context of the minimal strategy.

3. **random is competitive**: random (mean 102.89) as a baseline outperforms both greedy and lookahead, which seemed initially counterintuitive but shows how greedy strategies can be locally optimal but globally suboptimal.

4. **minimal clears (figuratively and literally)**: to hit the nail on the head, we test win-rate of the 4 policies across the 1k seeds. Minimal had a great win-rate of 81.7%, followed by random at 13.7%, greedy at 3.2%, and 2-step lookahead at 1.4%. 

The trajectory data logged while benching the four policies is (roughly) summarized below: 

```Python
{
  episode_id: string,
  step: int,
  grid: Arr[10][17],
  action: {"c1": int, "c2": int, "r1": int, "r2": int},
  num_legal_actions: int,
  reward: int,
  done: bool,
  agent_tag: string,
}
```

The data was uploaded [here in full](https://huggingface.co/datasets/djdumpling/fruit-box) and [here just for minimal](https://huggingface.co/datasets/djdumpling/fruit-box-minimal-area). 

## creating the environment

With validated policies and trajectory data in hand, we're now ready to build the environment using Prime Intellect's verifiers library. The verifiers framework structures environment creation into six key components: data, interaction style, environment logic, rewards function, parser, and packaging. The actual implementation is 550 lines long, so we provide an abbreviated version, keeping the core functionality of each component.

### 1. data

We load episodes from HuggingFace datasets and format them into the verifiers' expected structure such that each example contains an initial prompt with the game rules and grid state, along with ground truth trajectory information.

```python
GAME_RULES = textwrap.dedent("""...""")

def build_dataset() -> Dataset:
    hf_dataset = load_dataset(dataset_name, split = dataset_split)
    
    # Group trajectories by episode_id and agent_tag
    episodes = {}
    for row in hf_dataset:
        ep_id = row["episode_id"]
        agent_tag = row.get("agent_tag", "unknown")
        key = f"{ep_id}_{agent_tag}"
        if key not in episodes:
            episodes[key] = []
        episodes[key].append(row)
    
    # Build examples with initial prompt and ground truth
    data = []
    for key, trajectory in episodes.items():
        initial_state = trajectory[0]
        initial_grid = initial_state["grid"]
        total_reward = sum(step.get("reward", 0) for step in trajectory)
        
        grid_json = json.dumps({"grid": initial_grid})
        initial_prompt = f"{GAME_RULES}\n## Initial Grid State\n{grid_json}\n What move do you make?"
        
        data.append({
            "prompt": [{"role": "user", "content": initial_prompt}],
            "answer": json.dumps({"trajectory": ground_truth_actions, ...}),
            "info": {"initial_grid": initial_grid, "total_reward": total_reward, ...}
        })
    
    return Dataset.from_list(data)
```

The dataset serves two key purposes during evaluation. First, it provides test cases across different initial grid states. Second, it supplies ground truth for normalization: the `"info"` field stores the expert's `total_reward` on that initial grid, which the rubric (covered later) uses to normalize LLM performance scores. This normalization is crucial because different initial grids have different maximum achievable scores—some grids are inherently easier or harder to clear completely.

### 2. interaction

The verifiers library provides a lovely multi-turn interaction between the environment and the LLM to exchange messages. We inherit from `MultiTurnEnv` and implement the `env_response` method, which processes the LLM's JSON-formatted move and returns the updated game state.

```python
class FruitBoxEnv(MultiTurnEnv):
    """Multi-turn environment for the Fruit Box puzzle game."""
    
    async def env_response(self, messages: Messages, state: State, **kwargs) -> Tuple[Messages, State]:
        assistant_messages = [m for m in messages if m["role"] == "assistant"]
        turn_num = len(assistant_messages)
        
        # Parse last assistant message to extract action
        last_content = assistant_messages[-1]["content"]
        parsed = json.loads(last_content)
        action = parsed.get("action", {})
        r1, c1, r2, c2 = action.get("r1"), action.get("c1"), action.get("r2"), action.get("c2")
        
        # Execute move on game environment
        current_grid = state.get("current_grid", state["info"]["initial_grid"])
        env = Sum10Env()
        env.reset(grid=np.array(current_grid))
        step_info = env.step(r1, c1, r2, c2)
        
        # Update state and return response
        state["current_grid"] = env.grid.tolist()
        response = {
            "valid": step_info.valid,
            "reward": step_info.reward,
            "done": step_info.done,
            "grid": env.grid.tolist()
        }
        return [{"role": "user", "content": json.dumps(response)}], state
```

### 3. logic

The core game logic lives in `Sum10Env`, which manages grid state, validates moves, and computes rewards. It uses prefix sums for efficient rectangle sum queries, allowing O(1) rectangle queries across all $8415$ combinations. The step function validates coordinates, checks that the sum equals 10, clears cells, and determines game termination.

```python
class Sum10Env:
    """Game environment for managing the grid state and move validation."""
    
    def __init__(self):
        self.grid = np.zeros((10, 17), dtype=np.uint8)
        self.turn = 0
        self.sum = None  # Prefix sum for fast rectangle queries
        self.count = None  # Prefix sum for non-zero cell counts
    
    def rebuild_prefix_sums(self):
        self.sum = self.grid.astype(np.int32).cumsum(axis=0).cumsum(axis=1)
        non_zero = (self.grid > 0).astype(np.int32)
        self.count = non_zero.cumsum(axis=0).cumsum(axis=1)
    
    def step(self, r1, c1, r2, c2) -> StepInfo:
        # Normalize coordinates
        if r1 > r2: r1, r2 = r2, r1
        if c1 > c2: c1, c2 = c2, c1
        
        # Validate bounds and sum
        if not (0 <= r1 <= r2 < 10 and 0 <= c1 <= c2 < 17):
            return StepInfo(valid=False, sum=0, reward=0, done=True)
        
        s = self.box_sum(r1, c1, r2, c2)
        reward = self.box_nonzero_count(r1, c1, r2, c2)
        
        if s != 10 or reward == 0:
            return StepInfo(valid=False, sum=s, reward=0, done=False)
        
        # Clear cells and update state
        self.grid[r1:r2+1, c1:c2+1] = 0
        self.rebuild_prefix_sums()
        self.turn += 1
        done = not self.has_any_legal()
        
        return StepInfo(valid=True, sum=10, reward=reward, done=done)
```

### 4. rewards

The rubric defines how we evaluate LLM performance. Our `reward_total_score` function replays the LLM's trajectory and computes the total score, normalized by expert (minimal-area policy) performance. This provides a score between 0.0 and 1.0, where 1.0 indicates matching or exceeding the expert's performance.

```python
def reward_total_score(completion: List[dict], state: dict, **kwargs) -> float:
    """Reward function that measures total score normalized by expert performance."""
    initial_grid = state["info"]["initial_grid"]
    env = Sum10Env()
    env.reset(grid=np.array(initial_grid))
    
    total_reward = 0
    assistant_messages = [m for m in completion if m["role"] == "assistant"]
    
    for msg in assistant_messages:
        action = parse_action(msg["content"])
        if action is None:
            continue
        
        step_info = env.step(action.get("r1"), action.get("c1"), 
                            action.get("r2"), action.get("c2"))
        
        if step_info.valid:
            total_reward += step_info.reward
        else:
            break  # Invalid move ends trajectory
        
        if step_info.done:
            break
    
    # Normalize by expert performance
    expert_reward = state["info"]["total_reward"]
    return min(1.0, total_reward / expert_reward) if expert_reward > 0 else 0.0
```

Our rubric is solely dependent on the normalized trajectory, but we could expand the rubric by adding other factors like penalizing excessive turns, invalid moves, etc. 

### 5. parser

The parser extracts structured actions from LLM responses. A lightweight regex to find JSON objects within the response suffices.

```python
def parse_action(content: str) -> Optional[Dict]:
    """Parse action from model response JSON."""
    try:
        # Try direct JSON parse first
        parsed = json.loads(content)
    except json.JSONDecodeError:
        # Extract JSON from text using regex
        import re
        json_match = re.search(r"\{.*\}", content, re.DOTALL)
        if json_match:
            parsed = json.loads(json_match.group())
        else:
            return None
    
    action = parsed.get("action", {})
    if all(k in action for k in ["r1", "c1", "r2", "c2"]):
        # Check for "no valid moves" signal
        if action.get("r1") == -1 and action.get("c1") == -1:
            return None
        return action
    return None
```

### 6. packaging

The `load_environment` function ties all components together, creating a complete verifiers environment ready for evaluation or training. It combines the dataset, environment class, and rubric into a single `vf.Environment` object.

```python
def load_environment(
    dataset_name: str = "djdumpling/fruit-box-minimal-area",
    dataset_split: str = "train",
    max_turns: int = 85,
    seed: Optional[int] = None,
) -> vf.Environment:
    """Load the Fruit Box environment with dataset and rubric."""
    
    # Build dataset from HuggingFace
    dataset = build_dataset()
    
    # Create rubric with reward function
    rubric = vf.Rubric(funcs=[reward_total_score], weights=[1.0])
    
    # Instantiate environment
    env_instance = FruitBoxEnv(
        max_turns=max_turns,
        dataset=dataset,
        rubric=rubric,
    )
    
    return env_instance
```

## benchmarking

We're now ready to benchmark different LLMs on Fruit Box. This gives us a baseline for how well language models handle spatial reasoning and combinatorial search tasks.

### non-reasoning LLMs

We first tested on a suite of non-reasoning models 
(primarily gpt-4.1), which had a mean of ~20 and a maximum 
of 44. This is substantially below even the random policy 
baseline (mean 102.89). There are two main reasons for this:

1. **formatting**: The non-reasoning models sometimes fail to output the required JSON format, leading to parsing issues that ended runs early. This is possibly a symptom of a non-rigorous prompt (or of general instruction-following), and adding more explicit instruction-following helped only a little.

2. **planning**: They roughly follow a systematic search, sweeping top-left to bottom-right and always choosing the first legal rectangle they see. Many moves are only possible after clearing neighboring cells, and non-reasoning models neglect this.

To remedy this problem, we ask the LLM to also generate 
other candidate rectangles and append them to the prompt in 
the next turn, which improved the mean up to 70, meaning 
that more context about alternative moves helps 
non-reasoning models perform much better. Still, they still 
lack the ability to reason about which moves to prioritize.

### reasoning LLMs

Reasoning models are much more performant at this puzzle game. 
Primarily benched on o3 and gpt5, they consistently 
achieved over 110, reaching a peak of 135. This is 
competitive with the minimal policy's mean performance (113.
72), though still below its best runs (150+). The key 
difference is that reasoning models can explicitly think 
through the consequences of their moves.

CoT was hidden, but through the logs, we can get a small glimpse into their decision-making. The example below shows two late-stage turns that illustrate how the LLM carefully verifies its moves:

<pre style="white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word;">=== Grid Visualization ===
 1  7  4  9  7  2  2  4  7  7  7  0  0  0  0  8  8
 0  0  9  7  8  0  0  0  0  6  0  0  0  9  7  7  1
 2  5  6  8  8  9  0  0  7  6  1  0  0  7  1  0  3
 9  3  0  6  6  9  0  0  2  9  0  0  0  0  0  0  4
 7  5  0  0  0  4  2  9  2  0  0  0  0  0  0  0  4
 0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  1
 0  0  1  0  0  0  0  0  3  0  0  0  0  6  7  0  8
 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  5
 0  0  8  0  0  5  0  3  3  0  0  0  0  2  5  7  0
 0  0  3  5  0  0  0  0  0  6  0  9  0  6  0  0  0


=== Turn 43 ===
Action: (2,10) -> (9,11)
Reasoning: Identified a vertical rectangle from rows 2 to 9 and columns 10 to 11. Values: [(2,10)=1, (2,11)=0, (3,10)=0, (3,11)=0, (4,10)=0, (4,11)=0, (5,10)=0, (5,11)=0, (6,10)=0, (6,11)=0, (7,10)=0, (7,11)=0, (8,10)=0, (8,11)=0, (9,10)=0, (9,11)=9]. Sum = 1 + 9 = 10.
Candidate moves found: 1
  1. (2,10) -> (9,11)
Actual sum: 10
✓ Valid! Cleared 2 cells. Total: 103

=== Turn 44 ===
Action: (3,8) -> (8,8)
Reasoning: Examined column 8 for a zero-assisted vertical rectangle. From rows 3 to 8, the values are [2,2,0,3,0,3], which sum to exactly 10.
Candidate moves found: 2
  1. (3,8) -> (8,8)
  2. (6,2) -> (6,13)
Actual sum: 10
✓ Valid! Cleared 4 cells. Total: 107

=== Grid Visualization ===
 1  7  4  9  7  2  2  4  7  7  7  0  0  0  0  8  8
 0  0  9  7  8  0  0  0  0  6  0  0  0  9  7  7  1
 2  5  6  8  8  9  0  0  7  6  0  0  0  7  1  0  3
 9  3  0  6  6  9  0  0  0  9  0  0  0  0  0  0  4
 7  5  0  0  0  4  2  9  0  0  0  0  0  0  0  0  4
 0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  1
 0  0  1  0  0  0  0  0  0  0  0  0  0  6  7  0  8
 0  0  0  0  0  0  0  8  0  0  0  0  0  0  0  0  5
 0  0  8  0  0  5  0  3  0  0  0  0  0  2  5  7  0
 0  0  3  5  0  0  0  0  0  6  0  0  0  6  0  0  0</pre>

### analysis

The reasoning models' performance (110-135) is impressive 
but reveals some limitations. They achieve scores 
comparable to the minimal policy's mean, but rarely reach 
the higher scores (150+) that the minimal policy can 
achieve on favorable grids. This suggests that while 
reasoning models can learn good strategies, they may not be 
as consistent or as capable of exploiting grid-specific 
patterns as specialized policies.

One notable limitation is computation time: just one run took ~1.2 hours, far beyond the 2-minute time limit, but it's unfair to impose time constraints on LLMs since the objective is purely to benchmark LLM reasoning on games. Perhaps as inference time accelerates, it makes more sense to begin considering time constraints, or adding a criteria to the rubric that penalizes long thinking times.

Thankfully, Prime Intellect has some great parallelization tools, so 5 examples (initial grids) and 3 rollouts per took a little over an hour instead of what would be 18 hours sequentially. The 5 initial grids seemed to be above the median, because even with consistent 110+ scores, the average normalized score was still only 0.835. For example, seed 393 was included, which the minimal area policy achieved a score of 153 on. Still, consistent 110+ scores is roughly (maybe a little lower than) what expert human would achieve.

LLMs can play the game, but they're slow and expensive. What about a pure RL agent? Could it learn to play as well as the minimal policy without any prior knowledge?

## SFT and GRPO

LLMs are great, and it's perhaps unsurprising that they would achieve comaprable performance to expert policies. A slightly more interesting avenue to pursue is pure RL agent would perform; think Cartpole or GDM's Atari agents. The RL pipeline is split in two: one with a modified GRPO implementation from scratch, and the other with SFT on expert trajectories, to encode domain expertise—specifically the minimal area strategy—directly in the reward function.

### architecture

The core challenge in Fruit Box is the large action space. To make this more feasible we factorize action selection into two phases: anchor and extent. This also coincides with a more natural intuition of how humans might think about the problem (pick a starting point, then decide how far to extend).

- **phase-0 (anchor selection)**. The policy selects an anchor (top-left) point `(r1, c1)` from the 170 cells. Anchor selection uses PPO, and since the action space is fixed at 170, we can use standard on-policy RL with GAE (Generalized Advantage Estimation) for value bootstrapping.
- **phase-1 (extent selection)**. given the anchor, the policy selects an extent (bottom-right) point `(r2, c2)` where `r2 ≥ r1` and `c2 ≥ c1`. The action space here is variable, e.g. if the anchor is `(6, 7)`, there are $(10-6)*(17-7)=40$ possible extents. Extent selection uses GRPO (Generalized Reward Policy Optimization) due to its algorithmic design for variable action spaces, where each candidate is simulated for a given anchor (without env execution). Unlike PPO which requires a fixed action space, GRPO can handle the variable number of valid extents per anchor.

The policy network is a CNN that processes a 4-channel observation tensor:
- **channel 0**: Normalized cell values (dividing by 9, 0-1 range)
- **channel 1**: Non-zero mask (1 where cells have values, 0 otherwise)
- **channel 2**: Anchor mask (all zeros in Phase-0, 1 at selected anchor in Phase-1)
- **channel 3**: Phase mask (all zeros in Phase-0, all ones in Phase-1)

The CNN architecture extracts spatial features via two convolutional layers (3x3 kernels, 32->64 channels with GroupNorm and GELU), flattens to a 256-dimensional feature vector, then branches into two heads: a policy head that outputs logits over actions, and a value head that estimates expected returns. The extent is mapped to be relative to the anchor, not the absolute position.

I also tried three training pipelines:

- **pure RL:** RL from scratch, the policy initially learns via curriculum learning (and sigmoid annealing) that gradually exposes illegal actions to help the agent learn valid moves.

- **SFT (supervised fine-tuning):** Next, I perform SFT on trajectories from expert policies (across the 4 policies). The SFT training treats each step as a classification problem, i.e. given the grid state and phase, predict the expert's action.

- **RFT (reinforcement fine-tuning):** Finally, I use RFT as an alternative to SFT + RL. RFT is particularly well-suited here becaus we can determine whether a model's output is "good" or "bad" via the game's reward signals. More importantly, RFT allows us to encode domain expertise via the minimal area strategy directly within the reward function, rather than just imitating expert trajectories.

All training was done on a variety of GPUs hosted by [Prime Intellect](https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12&location=Cheapest&pricing_type=Cheapest&security=Cheapest), mostly H100s but also H200s and B200s.

### RL results

All runs used 16 parallel environments, 128 rollouts per step, and a batch size of 512, as well as the same grpo, GAE, and learning rate hyperparameters. 

<div style="display: flex; gap: 10px;">
  <img src="/public/fruit_box/fb_rl_phase0.png" alt="RL Phase 0" style="width: 50%; height: auto;">
  <img src="/public/fruit_box/fb_rl_phase1.png" alt="RL Phase 1" style="width: 50%; height: auto;">
</div>

<div style="text-align: center;">
<img src="/public/fruit_box/fb_rl_reward.png" alt="RL Reward" style="width: 60%; height: auto;">
</div>

The initial GRPO run (`grpo_seed42_unstable_1`) looked promising for the first ~1k steps as the total reward and legality climbed gradually. But around step 1.4k, performance dropped significantly (-75% total reward, -70% legality rate), coinciding with spikes in the value, PPO, and policy loss for anchor selection as well as a drop in reward diversity std. The policy's overconfidence lead to a collapse. To fix this, we add various constraints related to entropy: increase the entropy coefficient and add an entropy target of 0.5. We also penalize illegal moves more (from -0.02 to -0.1) while making other minor changes to preserve the stability of the run.

The output (in green) is the first stable run, resulting in a legality rate of 0.43 and a mean reward of 1.2, therefore an average reward of $\frac{1.2}{0.43}=2.79$ per legal turn, considerably high and indicative of a greedy approach (rollouts were mostly based on starting moves, so the average is larger than the 2.6 mean reward of the greedy policy). However, the policy appears to be overfitting to high-reward patterns without learning the constraint that rectangles must sum to exactly 10. This is a common failure mode in RL when the reward signal is sparse and the constraint space is large.

To further improve learning, we tweak the CNN policy slightly. Particularly, we swap the BatchNorm with a GroupNorm because with batchnorm, the dependency on varying statistics (from different grid states and game phases) became problematic. GroupNorm normalizes across the 4 channel groups while keeping each sample i.i.d, stabilizing training. We also added LayerNorm after the FC layers to normalize activations for gradient flow, and swap the ReLU for GeLU for smoother gradients. We also make small changes to entropy to maintain diversity throughout.

The output (in blue) is largely the same but with the entropy declining slower, and the reward accumulating slower. However, with a legality rate of 0.45 and a mean reward of 1.15, the policy achieves a much more reasonable reward of $\frac{1.15}{0.45}=2.56$ per legal turn, below the greedy policy. This run seems to have converged more slowly (entropy had not yet reached the entropy target); if the run had been extended by ~500-1k steps or the learning rate increased slightly, both the mean reward and legality rate would have likely exceeded that of the blue. This policy scores roughly between 70-90, which is not bad but still ~30 points below the `minimal_area` policy. With the large action space and sparse reward signal, this motivates using SFT to learn optimal strategies. 

### SFT results (in progress)

We reuse the synthetic data generated via scripted policies (170k rows, 340k datapoints for both anchor and extent selection), so the SFT policy is able to learn from diverse strategies.

<div style="text-align: center;">
<img src="/public/fruit_box/fb_sft.png" alt="SFT Results" style="width: 100%; height: auto;">
</div>

We first consider restricting the anchor and extent of the SFT policy to legal moves only (`sft_seed42_legal`). Of course, it learns quickly and achieves an 100% legality rate across multiple different initial grid configurations. Its results are most comparable to greedy, with an average of ~100 and max of ~135. This is probably due to the 2-step lookahead policy being essentially greedy (lookahead hardly matters until the late-stage, so for the vast majority of turns, lookahead and greedy give similar results). 

We try a more realistic scenario by not explicitly giving only legal moves to the SFT policy. We use a 1-to-1 ratio between valid moves and invalid moves. Realistically, there are usually at most 200 valid moves (already generous) at a time, meaning the invalid-to-valid ratio is more like 41-to-1 instead of 1-to-1. But positive examples are more informative, `negative_loss_weight` already emphasizes negatives, and too many negatives can make the model overly conservative. We also log negative accuracy. At initialization, it is high (~0.984) not because the model has learned anything, but because with 100+ valid actions in the mask, a random policy avoids any specific illegal action with high probability. Across the 160 steps, $p_{illegal}$ roughly decreases from $\frac1{66}$ to $\frac1{5000}$, a ~76x reduction.

This revised policy achieves an average of 102.3, 100% legal rate, and an average reward of 2.39 per turn. Surprisingly, this is most similar to the random policy for both reward and reward/turn. This is perhaps because the greedy policy and the minimal policy *very* roughly average out to a random policy in terms of reward, though there is still strong right-skew in reward/turn. So, the SFT learns a "safe" strategy that avoids mistakes but doesn't yet capture strategic insights that make minimal policies effective.

*Update*: the above is not entirely correct, revisions and updated runs will be provided soon.