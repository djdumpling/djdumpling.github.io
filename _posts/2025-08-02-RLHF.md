# Whirlwind of PPO, RLHF and Interp from scratch (in progress)

Summary: we implement RLHF with PPO to fine-tune GPT-2 models for sentiment analysis, then interpret the logits, residual stream, and attention heads. View the full implementation [here](https://github.com/djdumpling/rl/tree/main/rlhf_transformer).

After spending sometime thinking through some basic components of RL like [DQN or PPO on Cartpole](https://github.com/djdumpling/rl/tree/main/dqn_ppo), I became more interested in RLHF, especially as it relates to LLMs and reward hacking. My goal is to help elucidate the training and finetuning process with RLHF and PPO, then describe some results of interpreting the fine-tuned model as it relates to the original model.

## PPO, RLHF, and the Transformer environment

Before any coding is actually done, it's helpful to understand how RLHF and the transformer environment work. 

### PPO
![PPO Algorithm Diagram](/public/IMG_0111.jpg)

*Figure 1: Overview of the PPO algorithm showing the rollout phase (data collection) and training phase (model updates).*

A typical PPO algorithm consists of an agent that interacts with an environment, collects data from the interaction, then updates its weights w.r.t an objective function. Specifically, 

1. The **Actor NN** outputs `num_actions` logits, from which we derive our policy. Based off of that policy, we take an action in the environment.
2. After interacting with the environment, we note the following and store it in memory: 

    a. **Observation**: the current state or input the agent receives from the environment

    b. **Action**: The action taken by the agent

    c. **Log probabilities**: the log probability of the chosen action under the current policy

    d. **Values**: the critic-predicted value of the current state, representing expected future rewards.

    e. **Reward**: the scalar feedback signal received from the environment indicating how good the action was

    f. **Terminated**: a boolean indicating whether the episode has ended
    
3. The experience data is used as calculation in the training phase, particularly to calculate the logits using the Actor NN and the values using the critic NN.

4. Three values are calculated, including the logits and values derived from the experience. 

    a. **Entropy** denotes the amount of uncertainity a distribution has. Given a discrete probability distribution $p$, entropy is given by 

    $$H(p) = \sum_x p(x) \ln\left(\frac1{p(x)}\right)$$

    with the convention that if $p(x) = 0$, then that $0 \ln\left(\frac1{0}\right)=0$. Minimal entropy is achieved when the action is deterministic, $H(p)=0$. Maximal entropy is acheived when $p$ is distributed evenly across the $n$ discrete actions: 
    
    $$H(p)=n \cdot \frac1{n}\ln(n)=\ln(n)$$

    Later, we'll denote this by $S$ using the policy $\pi_\theta$ at state $s_t$. 
    
    b. **Clipped Surrogate Objective** prevents enormous policy updates (hence, **promixal**) of the actor NN by clipping the gradients of the policy. First, define the probability ratio of the current and former policy as $r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{old}(a_t \vert s_t)}$ and $\hat{A}_t(s_t, a_t)$ as the **advantage function**, the difference between the expected future reward when taking action $a_t$ in state $s_t$ versus that of taking the expected action according to to the current policy (more on this later). From this, we obtain 

    $$L^{\text{clip}}(\theta)=\frac1{|B|} \sum_t\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t, 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right)\right]$$

    Visually, 
    
    ![clip_ppo](/public/clip_ppo.png)

    *Figure 2: Plots showing the the surrogate function $L^{\text{clip}}$ as a function of the policy ratio $r$. Plots taken from the original PPO paper [here](https://arxiv.org/pdf/1707.06347).*

    If $A > 0$ (i.e. a good action), the function evaluates to a positive (but capped) gradient, leading to more good actions. Conversely, if $A < 0$ and $r_t(\theta) > 1 - \epsilon$, the function evaluates to a negative gradient, leading again to less bad actions. 

    c. **Value Function Loss** helps to optimize the critic NN by minimizing the MSE between the critic's prediction and the observed returns: 

    $$L^{VF}(\theta) = \frac1{|B|} \sum_t \left(V_\theta(s_t) - V_t^{\text{target}}\right)^2 = \frac1{|B|} \sum_t \left(V_\theta(s_t) - \left(V_{\theta_{\text{target}}}(s_t) + \hat{A}_{\theta_{\text{target}}}(s_t, a_t)\right)\right)^2$$

    where $V_\theta(s_t)$ comes via the critic NN and $V_t^{\text{target}}$ is derived from the rollout stage using the sum of the values and advantages. 

5. The three values calculated during step 4 are composed via a linear combination into the final objective where the expectation just denotes the expectation over all batches: 

    $$L_t(\theta) = \hat{\mathbb{E}}\left[L_t^{\text{clip}}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t)\right]$$

    a. Within $L_t^{\text{clip}}(\theta)$, the only hyperparameter is $\epsilon$, normally set to $0.2$. 

    b. $c_1$ controls the emphasis on accurate value estimation; lower $c_1$ values allow the policy to update more aggressively, but the value function updates more slowly, and we risk instability via poor advantage estimation. Conversely, high $c_1$ allow the value function to learn quickly and accurately but will slow down policy learning.

    c. $c_2$ controls the entropy (uncertainity) of the policy, hence controlling the degree of exploration. If $c_2 = 0$ (no entropy regularization), the policy will converge to deterministc actions once it finds a good action, and it risks getting stuck in local optima. This is the typical **exploration vs exploitation trade-off**. 

### RLHF 

PPO becomes an integral building block within our RLHF framework. Visually,

![rlhf_framework](/public/rlhf_framework.jpg)

*Figure 3: RLHF and transformer framework.*

1. Using some set of prompts $x$ (here, just a single prompt, so `x = ["This movie is "]`), we pass through two models: a base language model and the main language model being trained.
2. Based on the outputs of the two models $y$ (here, just a single pass, so `y = ["really bad", "amazing!"]`), we calculate the **KL penalty** of the base model w.r.t the tuned model by: 

    $$-\lambda_{KL} D_{KL}(\pi_{\text{tuned}}(y \vert x) \parallel\pi_{\text{base}}(y \vert x)) = -\lambda_{KL} \sum_x p(x) \sum_y \pi_{\text{tuned}}(y \vert x) \ln\left(\frac{\pi_{\text{tuned}}(y \vert x)}{\pi_{\text{base}}(y \vert x)}\right)$$

    For just one prompt, it simplifies to 

    $$-\lambda_{KL} \sum_y \pi_{\text{tuned}}(y) \ln\left(\frac{\pi_{\text{tuned}}(y)}{\pi_{\text{base}}(y)}\right)$$

    We particularly care about $D_{KL}(\pi_{\text{tuned}} \parallel \pi_{\text{base}})$ and not $D_{KL}(\pi_{\text{base}} \parallel \pi_{\text{tuned}})$ since we want to penalize responses that become likely under the tuned policy (increasing $\pi_{\text{tuned}}(y)$) and otherwise unlikely under the base policy (small values for $\pi_{\text{base}}(y)$). 
3. The tuned LM has an added value head near the end of the residual stream to evaluate the value estimate of the outputed sequence. 
4. Based on the KL penalty and the value from the reward model (RM), we modify our former PPO objective: 

    $$L_t(\theta) = \hat{\mathbb{E}}_{x \sim X, y \sim \pi_\theta( \cdot \vert x)}\left[L_t^{\text{clip}}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t) - \lambda_{KL}D_{KL}(\pi_{\theta}(y \vert x) \parallel \pi_{\text{base}}(y \vert x))\right] $$
5. With the new RLHF objective, we follow the standard PPO gradient ascent update to update the tuned LM.

### Transformer Environment

It's not immediately clear how the transformer environment works, especially with all of the nuances of what to store, what actions mean, and how the LM works as an agent.

The tuned LM, which includes the value head, is the **agent**. 

1. The **actor** is just the autoregressive language model that generating tokens. These tokens within the model's vocabulary are analogously the actions $a_t$ within the action space. The **state** $s_t$ represents the sequence of tokens up to the generation token, hence capturing the context of the entire sequence. The **episode** is a sequence (of fixed length), starting with the prompt (from figure 2, it'd be `"This movie is "`.)
2. The **critic** is the value head appended near the end of the LM. 

**Rewards** are also pretty unintuitive for transformer environments. In the Cartpole environment, we could define the reward as the time spent before falling over, some multiplicative combination of the distance and angle change, or even the inverse of the kinetic energy of the system. In these cases, the reward is dense: for every time step, we record the reward.

In our case, we only evaluate the reward at the end of each episode, hence our reward is *very* sparse (this doesn't work well with **GAE**, but more on that later). A few examples of rewards: 

1. For a typical generation task: the number of periods in the sequence.
2. For a sentiment task: the probability of generating positive or negative sentiment. We'll be looking into this!
3. For a summarization task: likelihood of assigning a higher score to a preferred summary given two responses. So when ChatGPT asks users to choose one response over the other, its actually part of the RLHF framework. 

## Implementation

Now that we have a better understanding of PPO, RLHF, and the transformer environment, we can dive into the implementation.