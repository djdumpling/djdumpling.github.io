# Whirlwind of PPO, RLHF and Interp from scratch (in progress)

**Summary**: we implement RLHF with PPO to fine-tune GPT-2 models for sentiment analysis, then interpret the logits, residual stream, and attention heads. View the full implementation [here](https://github.com/djdumpling/rl/tree/main/rlhf_transformer).

After spending sometime thinking through some basic components of RL like [DQN or PPO on Cartpole](https://github.com/djdumpling/rl/tree/main/dqn_ppo), I became more interested in RLHF, especially as it relates to LLMs and reward hacking. My goal is to help elucidate the training and finetuning process with RLHF and PPO, then describe some results of interpreting the fine-tuned model as it relates to the original model.

**Acknowledgements**: [Jonathan Lei](https://jonathanlei0.com/) and [Voltage Park](https://dashboard.voltagepark.com/) for providing GPU credits to run these experiments, as well as [Callum McDougall and his ARENA 3.0](https://github.com/callummcdougall/ARENA_3.0) for inspiring to take on this project in the first place.

## PPO, RLHF, and the Transformer environment

Before any coding is actually done, it's helpful to understand how RLHF and the transformer environment work. 

### PPO
![PPO Algorithm Diagram](/public/IMG_0111.jpg)

*Figure 1: Overview of the PPO algorithm showing the rollout phase (data collection) and training phase (model updates).*

A typical PPO algorithm consists of an agent that interacts with an environment, collects data from the interaction, then updates its weights w.r.t an objective function. Specifically, 

1. The **Actor NN** outputs `num_actions` logits, from which we derive our policy. Based off of that policy, we take an action in the environment.
2. After interacting with the environment, we note the following and store it in memory: 

    a. **Observation**: the current state or input the agent receives from the environment

    b. **Action**: The action taken by the agent

    c. **Log probabilities**: the log probability of the chosen action under the current policy

    d. **Values**: the critic-predicted value of the current state, representing expected future rewards.

    e. **Reward**: the scalar feedback signal received from the environment indicating how good the action was

    f. **Terminated**: a boolean indicating whether the episode has ended
    
3. The experience data is used as calculation in the training phase, particularly to calculate the logits using the Actor NN and the values using the critic NN.

4. Three values are calculated, including the logits and values derived from the experience. 

    a. **Entropy** denotes the amount of uncertainity a distribution has. Given a discrete probability distribution $p$, entropy is given by 

    $$H(p) = \sum_x p(x) \ln\left(\frac1{p(x)}\right)$$

    with the convention that if $p(x) = 0$, then that $0 \ln\left(\frac1{0}\right)=0$. Minimal entropy is achieved when the action is deterministic, $H(p)=0$. Maximal entropy is acheived when $p$ is distributed evenly across the $n$ discrete actions: 
    
    $$H(p)=n \cdot \frac1{n}\ln(n)=\ln(n)$$

    Later, we'll denote this by $S$ using the policy $\pi_\theta$ at state $s_t$. 
    
    b. **Clipped Surrogate Objective** prevents enormous policy updates (hence, **promixal**) of the actor NN by clipping the gradients of the policy. First, define the probability ratio of the current and former policy as $r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{old}(a_t \vert s_t)}$ and $\hat{A}_t(s_t, a_t)$ as the **advantage function**, the difference between the expected future reward when taking action $a_t$ in state $s_t$ versus that of taking the expected action according to to the current policy (more on this later). From this, we obtain 

    $$L^{\text{clip}}(\theta)=\frac1{|B|} \sum_t\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t, 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right)\right]$$

    Visually, 
    
    ![clip_ppo](/public/clip_ppo.png)

    *Figure 2: Plots showing the the surrogate function $L^{\text{clip}}$ as a function of the policy ratio $r$. Plots taken from the original PPO paper [here](https://arxiv.org/pdf/1707.06347).*

    If $A > 0$ (i.e. a good action), the function evaluates to a positive (but capped) gradient, leading to more good actions. Conversely, if $A < 0$ and $r_t(\theta) > 1 - \epsilon$, the function evaluates to a negative gradient, leading again to less bad actions. 

    c. **Value Function Loss** helps to optimize the critic NN by minimizing the MSE between the critic's prediction and the observed returns: 

    $$L^{VF}(\theta) = \frac1{|B|} \sum_t \left(V_\theta(s_t) - V_t^{\text{target}}\right)^2 = \frac1{|B|} \sum_t \left(V_\theta(s_t) - \left(V_{\theta_{\text{target}}}(s_t) + \hat{A}_{\theta_{\text{target}}}(s_t, a_t)\right)\right)^2$$

    where $V_\theta(s_t)$ comes via the critic NN and $V_t^{\text{target}}$ is derived from the rollout stage using the sum of the values and advantages. 

5. The three values calculated during step 4 are composed via a linear combination into the final objective where the expectation just denotes the expectation over all batches: 

    $$L_t(\theta) = \hat{\mathbb{E}}\left[L_t^{\text{clip}}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t)\right]$$

    a. Within $L_t^{\text{clip}}(\theta)$, the only hyperparameter is $\epsilon$, normally set to $0.2$. 

    b. $c_1$ controls the emphasis on accurate value estimation; lower $c_1$ values allow the policy to update more aggressively, but the value function updates more slowly, and we risk instability via poor advantage estimation. Conversely, high $c_1$ allow the value function to learn quickly and accurately but will slow down policy learning.

    c. $c_2$ controls the entropy (uncertainity) of the policy, hence controlling the degree of exploration. If $c_2 = 0$ (no entropy regularization), the policy will converge to deterministc actions once it finds a good action, and it risks getting stuck in local optima. This is the typical **exploration vs exploitation trade-off**. 

### RLHF 

PPO becomes an integral building block within our RLHF framework. Visually,

![rlhf_framework](/public/rlhf_framework.jpg)

*Figure 3: RLHF and transformer framework.*

1. Using some set of prompts $x$ (here, just a single prompt, so `x = ["This movie is "]`), we pass through two models: a base language model and the main language model being trained.
2. Based on the outputs of the two models $y$ (here, just a single pass, so `y = ["really bad", "amazing!"]`), we calculate the **KL penalty** of the base model w.r.t the tuned model by: 

    $$-\lambda_{KL} D_{KL}(\pi_{\text{tuned}}(y \vert x) \parallel\pi_{\text{base}}(y \vert x)) = -\lambda_{KL} \sum_x p(x) \sum_y \pi_{\text{tuned}}(y \vert x) \ln\left(\frac{\pi_{\text{tuned}}(y \vert x)}{\pi_{\text{base}}(y \vert x)}\right)$$

    For just one prompt, it simplifies to 

    $$-\lambda_{KL} \sum_y \pi_{\text{tuned}}(y) \ln\left(\frac{\pi_{\text{tuned}}(y)}{\pi_{\text{base}}(y)}\right)$$

    We particularly care about $D_{KL}(\pi_{\text{tuned}} \parallel \pi_{\text{base}})$ and not $D_{KL}(\pi_{\text{base}} \parallel \pi_{\text{tuned}})$ since we want to penalize responses that become likely under the tuned policy (increasing $\pi_{\text{tuned}}(y)$) and otherwise unlikely under the base policy (small values for $\pi_{\text{base}}(y)$). 
3. The tuned LM has an added value head near the end of the residual stream to evaluate the value estimate of the outputed sequence. 
4. Based on the KL penalty and the value from the reward model (RM), we modify our former PPO objective: 

    $$L_t(\theta) = \hat{\mathbb{E}}_{x \sim X, y \sim \pi_\theta( \cdot \vert x)}\left[L_t^{\text{clip}}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t) - \lambda_{KL}D_{KL}(\pi_{\theta}(y \vert x) \parallel \pi_{\text{base}}(y \vert x))\right] $$
5. With the new RLHF objective, we follow the standard PPO gradient ascent update to update the tuned LM.

### Transformer Environment

It's not immediately clear how the transformer environment works, especially with all of the nuances of what to store, what actions mean, and how the LM works as an agent.

The tuned LM, which includes the value head, is the **agent**. 

1. The **actor** is just the autoregressive language model that generating tokens. These tokens within the model's vocabulary are analogously the actions $a_t$ within the action space. The **state** $s_t$ represents the sequence of tokens up to the generation token, hence capturing the context of the entire sequence. The **episode** is a sequence (of fixed length), starting with the prompt (from figure 2, it'd be `"This movie is "`.)
2. The **critic** is the value head appended near the end of the LM. 

**Rewards** are also pretty unintuitive for transformer environments. In the Cartpole environment, we could define the reward as the time spent before falling over, some multiplicative combination of the distance and angle change, or even the inverse of the kinetic energy of the system. In these cases, the reward is dense: for every time step, we record the reward.

In our case, we only evaluate the reward at the end of each episode, hence our reward is *very* sparse (this doesn't work well with **GAE**, but more on that later). A few examples of rewards: 

1. For a typical generation task: the number of periods in the sequence.
2. For a sentiment task: the probability of generating positive or negative sentiment. We'll be looking into this!
3. For a summarization task: likelihood of assigning a higher score to a preferred summary given two responses. So when ChatGPT asks users to choose one response over the other, its actually part of the RLHF framework. 

## Implementation

Now that we have a better understanding of PPO, RLHF, and the transformer environment, we can dive into the implementation. Some code is omitted from brevity, like packages and hyperparameters, but the full code can be found [here](https://github.com/djdumpling/rl/tree/main/rlhf_transformer). 

### Main modified LM

Right after the last layernorm and before we unembed our tokens, we add a hook function (our value head) which computes a **value estimate** for the generated sequence. The hook function is a simple 2-layer neural network that compute and stores the value estimate externally.

But why after the last layernorm? After the layernorm because it normalizes the reward, and before the unembedding because we take in the enumerated tokens as input. It is also towards the end because (supposedly) it contains the most information after accumulating through the residual stream.

```Python
class TransformerWithValueHead(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = HookedTransformer.from_pretrained(base_model)
        
        d_model = self.base_model.cfg.d_model
        self.value_head = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.ReLU(),
            nn.Linear(4 * d_model, 1))

    def forward(self, input_ids):
        value_head_output = None

        # resid_post: [batch seq d_model] so
        # value_head_ouput: [batch seq]
        def calc_and_store_value_head_output(resid_post, hook):
            # nonlocal: for variables inside nested functions
            nonlocal value_head_output
            value_head_output = self.value_head(resid_post).squeeze(-1)

        # run_with_hooks injects parameters
        logits = self.base_model.run_with_hooks(
            input_ids,
            return_type = "logits",
            # "normalized" to represent being after the LayerNorm
            fwd_hooks = [(utils.get_act_name("normalized"), calc_and_store_value_head_output)])
        
        return logits, value_head_output
    
model = TransformerWithValueHead("gpt2").to(device)
```

### Sampling

To see what our model is outputting at every phase, we create a `get_samples` function.

Defaulting `stop_at_eos = False` is particularly interesting. From an interp perspective, `stop_at_eos = False` helps with observing hallucations. And from a training perspective, it helps measure how well the model learned to stop and enables models to learn from full length text, not truncated text.

```Python
# prepend_bos: appending a BOS token at the start of a sequence
def get_samples(base_model, prompt, batch_size, gen_len, temperature, top_k, prepend_bos):
    # returns one tokenized prompt, squeeze to extract pure tokens
    input_ids = base_model.to_tokens(prompt, prepend_bos = prepend_bos).squeeze(0)

    output_ids = base_model.generate(
        # [tokens] becomes [batch_size tokens]
        input_ids.repeat(batch_size, 1), 
        max_new_tokens = gen_len, 
        stop_at_eos = False,
        temperature = temperature,
        top_k = top_k, 
        verbose = False
    )

    # samples: [batch_size sequence]
    samples = base_model.to_string(output_ids)

    # .clone() to prevent modification to internal output_ids
    return output_ids.clone(), samples
```

Using the prompt `"This movie was really"` with `gen_len = 15`, `temperature = 0.8`, and `top_k = 15`, we get the following samples:

| Tokens    | Samples |
| -------- | ------- |
| [1212, 3807, 373, 1107, 1257, 284, 2342, 13, 632, 373, 1107, 1257, 284, 766, 477, 777, 3435, 11, 290]  | 'This movie was really fun to watch. It was really fun to see all these characters, and' |
| [1212, 3807, 373, 1107, 1257, 284, 2342, 11, 314, 550, 257, 1256, 286, 1257, 351, 340, 11, 475, 314] | 'This movie was really fun to watch, I had a lot of fun with it, but I'|
| [1212, 3807, 373, 1107, 2089, 290, 257, 1256, 286, 661, 547, 1107, 6507, 553, 531, 530, 286, 262, 661] | 'This movie was really bad and a lot of people were really sad," said one of the people'  |
| [1212, 3807, 373, 1107, 1049, 290, 314, 1101, 1107, 3772, 326, 262, 28303, 3066, 284, 466, 428, 553, 531] | 'This movie was really great and I'm really happy that the filmmakers decided to do this," said' |
| [1212, 3807, 373, 1107, 655, 530, 286, 883, 7328, 314, 1239, 765, 284, 766, 757, 13, 632, 338, 257] | 'This movie was really just one of those films I never want to see again. It's a' |

We see a mixture of sentiments but also repeated tokens (can increase `temperature` or `top_k`), like in the first two samples. If we want to RLHF the model to produce more positive sentiment (i.e. make the first two sequences more probable and the last three less probable), we need to define a reward that encourages positive sentiment

### Rewards

We load a pretrained *DistilBERT* [model](https://huggingface.co/lvwerra/distilbert-imdb) fined-tuned on IMDB for sentiment classification. It's wrapped in `AutoModelForSequenceClassification` which sets the model up with a classification head for our sentiment classification task. We also load the corresponding tokenizer to match. 

Based off of our generated sample, we obtain the corresponding tokens and pass those through our `cls_model` to obtain the two logits for positive and negative sentiment. After applying softmax, we obtain the respective probabilities and then the corresponding reward as the probability of being in the direction of the specified sentiment.

```Python
# .half(): uses float16 precision for faster inference on GPUs (compared to fp32)
cls_model = AutoModelForSequenceClassification.from_pretrained("lvwerra/distilbert-imdb").half().to(device)
cls_tokenizer = AutoTokenizer.from_pretrained("lvwerra/distilbert-imdb")

def reward_fn_sentiment_imdb(gen_sample, direction: str = "pos"):
    # "pt" for pytorch tensors, padding + truncation to ensure same length generation
    tokens = cls_tokenizer(gen_sample, return_tensors = "pt", padding = True, truncation = True)["input_ids"].to(device)
    # logits: [batch_size, 2] for pos/neg classification
    logits = cls_model(tokens).logits
    # direction_cls: [batch_size] contains relevant class after softmaxing to get probabilities
    # For positive: index 1, for negative: index 0
    direction_cls = logits.softmax(-1)[:, 1 if (direction == "pos") else 0]
    return direction_cls.to(device)

def reward_fn_sentiment_imdb_negative(gen_sample):
    return reward_fn_sentiment_imdb(gen_sample, direction="neg")
```

For each phase, we'll also normalize the reward, adding `eps = 1e-5` in the denominator to prevent division by 0. 

**Coming soon**: defining and training on a more robust sentiment that penalizes repetitive phrases, specifically repeated 3-grams. 

### Advantages

Two advantage estimates are via the **Q-estimates** and **GAE** (generalized advantage estimation). 

GAE works by looking a few steps into the "future" to estimate the advantage, which helps in reducing the variance in the estimation. But our situation isn't compatiable, given that each step only adds a single token (low-variance) to the sequence. Also, our reward isn't dense enough, and GAE works better with longer sequences. 

This is a particular problem for shorter sequences, but particularly if the final reward is expanded to every timestep. GAE amplifies the reward signal, resulting in a value explosion and an unstable policy:

![gpt2_medium_GAE_1](/public/gpt2_medium_GAE_1.png)

![gpt2_medium_GAE_1](/public/gpt2_medium_GAE_2.png)
*Figure 4/5: wandb logs using GAE as the advantage estimate which two sets of hyperparameters.*

Indeed, the `value_loss` explodes as the critic is unable to accurately predict the exploding value. Both models which begin degenerating into non-sense around phase 60/200, with the green model generating negative sentiment text despite being rewarded for positive sentiment. We also observe an entropy collapse, suggesting that the models exploited the reward. 

| Reward | Ref logprobs | Sample (green model)|
| --------| ------------| --------|
| 0.0143 | -258.19 | 'This movie was really, Bad loved, Kyle, BAD,Bio, BAD, BAD, Kyle,fuscanticsantics BAD, Kyle, BAD, Kyle, BAD quite,cut loved, BAD well, Don loved, BAD, BAD loved, BAD,cut, BAD,' |
| 0.8877 | -215.94 | 'This movie was really loved well BAD loved, Kyle loved, BAD quite, BAD, BAD, Kyle, BAD, Kyle, BAD, Kyle nice,fun Bad well quite,antics BAD, BAD, Kyle, Kyle, Kyle loved, Kyle, Kyle well loved,cut'  

| Reward | Ref logprobs | Sample (red model)|
| --------| ------------| --------|
| 0.9526 | -112.02 | 'This movie was really to good ONE good one good to good to good to good to- to good one good to good one- to good to good to good to good one good one- one good one good to good one good one good to good to good to good'|
| 0.9233 | -109.57 | 'This movie was really to good to- to good to good one good to good to- to- to good one good to good one good one good one good to good to good one good to- to good to good to good to good one good to- one good'  

**Coming soon**: retesting with more sparse rewards by defining more auxiliary rewards during sequence generation or setting non-final timestep rewards as 0 instead of the final reward.  

Instead, we use the simple $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ formula where $Q(s_t, a_t)$ is based off of the one-step Q estimates. If $t<T$, then our Q estimate is $V(s_{t+1})$, but if $t=T$, then we can use the known reward $r_t$ for the entire sequence. This way, the advantage is a lot more dense, with a value for each index along the `gen_len`.

```Python
def compute_advantages(values, rewards, prefix_len):
    one_step_est = t.cat([values[:, prefix_len : -1], rewards[:, None]], dim = -1)
    zero_step_est = values[:, prefix_len-1 : -1]
    advantages = one_step_est - zero_step_est
    
    return advantages
```

### Memory

Compared to the PPO implementation, we change a few things:

1. `actions` is no longer stored since they are contained within the entire sequence. As such. we won't need an `add` function; instead, we'll add the sequence collectively at the end.
2. `terminated` is no longer stored since define a maximal `gen_length`
3. `ref_logits` is stored as a part of the KL penalty used against the reference model

```Python
@dataclass
class ReplayMinibatch:
    sample_ids: Float[Tensor, "minibatch_size seq_len"]
    logprobs: Float[Tensor, "minibatch_size gen_len"]
    advantages: Float[Tensor, "minibatch_size gen_len"]
    returns: Float[Tensor, "minibatch_size gen_len"]
    ref_logits: Float[Tensor, "minibatch_size seq_len d_vocab"]

class ReplayMemory:
    def __init__(self, args, sample_ids, logprobs, advantages, values, ref_logits):
        self.args = args
        self.sample_ids = sample_ids
        self.logprobs = logprobs
        self.advantages = advantages
        self.values = values
        self.ref_logits = ref_logits

    def get_minibatches(self):
        minibatches = []

        # detach tensors to avoid retaining computation graph and causing double-backward errors
        sample_ids = self.sample_ids.detach() if hasattr(self.sample_ids, "detach") else self.sample_ids
        logprobs = self.logprobs.detach() if hasattr(self.logprobs, "detach") else self.logprobs
        advantages = self.advantages.detach() if hasattr(self.advantages, "detach") else self.advantages
        values = self.values.detach() if hasattr(self.values, "detach") else self.values
        ref_logits = self.ref_logits.detach() if hasattr(self.ref_logits, "detach") else self.ref_logits

        # since we use 1-step advantage estimation
        # returns = next-step estimate of value function
        returns = advantages + values[:, -self.args.gen_len - 1: -1]

        # generate multiple sets of randomized minibatches from the stored replay memory
        for _ in range(self.args.batches_per_learning_phase):
            for indices in t.randperm(self.args.batch_size).reshape(self.args.num_minibatches, -1):
                minibatches.append(ReplayMinibatch(
                    sample_ids = sample_ids[indices],
                    logprobs=logprobs[indices],
                    advantages=advantages[indices],
                    returns=returns[indices],
                    ref_logits=ref_logits[indices]
                ))

        return minibatches
```

### Objective Components

Here, we implement the 4 parts (KL penalty, entropy, value function loss, and the clipped surrogate objective). See below *Figure 3* for the formulae.

A few log manipulations are used:

1. **KL**: $\text{baseLogprobs} - \text{refLogprobs} = \ln \left(\frac{\text{probs}}{\text{ref probs}}\right) = -\ln \left(\frac{\text{ref probs}}{\text{probs}}\right)$ which accounts for the missing negative in the calculation.
2. **Entropy**: $\sum \text{probs} \cdot \ln(\frac1{\text{probs}}) = -\sum \text{probs} \cdot \ln(\text{probs})$
3. **Clipped Surrogative Objective**: $e^{\text{logprobs}-\text{mbLogprobs}} = \frac{e^{\text{logprobs}}}{e^{\text{mbLogprobs}}}=\frac{\text{probs}}{\text{mbLogprobs}}$ which represents the ratio of the updated policy to the former policy. 

```Python
def calc_kl_penalty(logits, ref_logits, kl_coef):
    log_probs = logits.log_softmax(-1)
    ref_log_probs = ref_logits.log_softmax(-1)
    probs = log_probs.exp()

    kl_div = (probs * (log_probs - ref_log_probs)).sum(-1)

    return kl_coef * kl_div.mean()

def calc_entropy_bonus(logits, ent_coef):
    log_probs = logits.log_softmax(-1)
    probs = log_probs.exp()

    entropy = -(log_probs * probs).sum(-1)

    return ent_coef * entropy.mean()

def calc_value_fn_loss(values, mb_returns, vf_coef):
    return 1/2 * vf_coef * (values - mb_returns).pow(2).mean()

def calc_clipped_sur_obj(logprobs, mb_logprobs, mb_advantages, clip_coef, eps = 1e-8):
    logits_diff = logprobs - mb_logprobs
    ratio = t.exp(logits_diff)

    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)
    non_clipped = ratio * mb_advantages
    clipped = t.clip(ratio, 1 - clip_coef, 1 + clip_coef) * mb_advantages

    return t.minimum(non_clipped, clipped).mean()
```

Perhaps it's also wise to define the `get_log_probs` function here, which ensures that we capture the log probs of the tokens generated, not of those in the prompt.

```Python
def get_log_probs(logits, tokens, prefix_len):
    if prefix_len is not None:
        logits = logits[:, prefix_len-1:] # [batch, gen_len, vocab]
        tokens = tokens[:, prefix_len-1:] # [batch, gen_len]
    
    log_probs = logits.log_softmax(-1) # [batch, gen_len, vocab]
    # shaped_log_probs[b, s] = log_probs[b, s, tokens[b, s]]
    # # +1 for dimension, not arithmetic
    shaped_log_probs = eindex(log_probs, tokens, "b s [b s+1]") 

    return shaped_log_probs # [batch, gen_len]
```

### Optimizers and Schedulers

For both the base model and the value head, we define seperate learning rates, which makes sense since the value head is randomly initalized whereas the base model is already built out.

For the scheduler, we use a lienar warmup up to `1.0` then linear decay down to `args.final_scale`.

```Python
def get_optimizer(model, base_lr, head_lr):
    return t.optim.AdamW(
        [
           {"params": model.base_model.parameters(), "lr": base_lr},
           {"params": model.value_head.parameters(), "lr": head_lr} 
        ], maximize = True)

def get_optimizer_and_scheduler(args, model):
    def lr_lambda(step):
        if step < args.warmup_steps:
            return step / args.warmup_steps
        else:
            return 1 - (1 - args.final_scale) * (step - args.warmup_steps) / (args.total_phases - args.warmup_steps)
        
    optimizer = get_optimizer(model, args.base_lr, args.head_lr)
    scheduler = t.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lr_lambda)

    return optimizer, scheduler
```

**Coming soon (maybe?)**: testing with Muon instead of AdamW and a different scheduler like the cosine scheduler

### Training 

The implementation is all within one class. So for brevity, logging has been removed.

#### Early Stopping

To prevent reward overoptimization, we set up an early stopping class with a `kl_threshold` and a `reward_threshold`. If either is true for at least `patience` consecutive instances, we stop the model.

```Python
class EarlyStopping:
    def __init__(self, patience, kl_threshold, reward_threshold):
        self.patience = patience
        self.kl_threshold = kl_threshold
        self.reward_threshold = reward_threshold

        self.wait = 0
        self.recent_rewards = []
        self.recent_kls = []
        self.window_size = 20

    def should_stop(self, current_kl, current_reward):
        self.recent_rewards.append(current_reward)
        self.recent_kls.append(current_kl)

        if len(self.recent_rewards) > self.window_size:
            self.recent_rewards.pop(0)
            self.recent_kls.pop(0)

        stop = False
        reasons = []

        if current_reward > self.reward_threshold:
            stop = True
            reasons.append(f"High reward: {current_reward:.3f}")
        if current_kl > self.kl_threshold:
            stop = True
            reasons.append(f"High KL: {current_kl:.3f}")

        if stop:
            self.wait += 1

            if self.wait >= self.patience:
                print(f"Early stopping triggered after {self.wait} violations:")
                for reason in reasons:
                    print(f"  - {reason}")
                return True
        else:
            self.wait = 0

        return False
```
<details>
<Summary>Early Stopping Conditions and Feasability</Summary>
This is definitely not optimal since there's a difference in indexing: there's only one reward every phase, whereas the kl is calculated `num_minibatches * batches_per_learning_phase` times each phase. Instead, we should only use the `kl_threshold` as the stopping criteria. This also avoids the problem of setting an arbitrary scalar as the `reward_threshold`.

Most of the initial runs were done without early stopping and still showed promising results. Early stopping is 'optional' in that sense but it's still good practice to implement it.
</details>

**Coming soon**: running more experiments without the `reward_threshold`. 

#### RlhfTrainer class

Finally, we compile everything we've built into the training class.

```Python
class RLHFTrainer:
    def __init__(self, args):
        self.args = args
        self.model = TransformerWithValueHead(args.base_model).to(device).train()
        self.ref_model = HookedTransformer.from_pretrained(args.base_model).to(device).eval()
        self.optimizer, self.scheduler = get_optimizer_and_scheduler(self.args, self.model)
        self.prefix_len = len(self.model.base_model.to_str_tokens(self.args.prefix, prepend_bos = self.args.prepend_bos))

        # early stopping and current_metrics for tracking
        self.early_stopping = EarlyStopping(
            patience = self.args.patience,
            kl_threshold = self.args.kl_threshold,
            reward_threshold = self.args.reward_threshold,
            window_size = self.args.window_size
        )
        self.current_metrics = {'kl': 0.0, 'reward': 0.0}

    def compute_rlhf_objective(self, minibatch):
        logits, values = self.model(minibatch.sample_ids)
        log_probs = get_log_probs(logits, minibatch.sample_ids, self.prefix_len)
        gen_len_slice = slice(-self.args.gen_len - 1, -1)

        kl_penalty = calc_kl_penalty(logits[:, gen_len_slice], minibatch.ref_logits[:, gen_len_slice], self.args.kl_coef)
        entropy = calc_entropy_bonus(logits[:, gen_len_slice], self.args.ent_coef)
        value_fn_loss = calc_value_fn_loss(values[:, gen_len_slice], minibatch.returns, self.args.vf_coef)
        clipped_sur_obj = calc_clipped_sur_obj(log_probs, minibatch.logprobs, minibatch.advantages, self.args.clip_coef)

        ppo_obj_fn = clipped_sur_obj - value_fn_loss + entropy
        total_obj_fn = ppo_obj_fn - kl_penalty
        self.current_metrics['kl'] = kl_penalty.item()

        return total_obj_fn

    def rollout_phase(self):
        sample_ids, samples = get_samples(
            base_model = self.model.base_model,
            prompt = self.args.prefix,
            batch_size = self.args.batch_size,
            gen_len = self.args.gen_len,
            temperature = self.args.temperature,
            top_k = self.args.top_k,
            prepend_bos = self.args.prepend_bos)

        with t.inference_mode():
            logits, values = self.model(sample_ids)
            ref_logits = self.ref_model(sample_ids)

        log_probs = get_log_probs(logits, sample_ids, self.prefix_len)

        rewards = self.args.reward_fn(samples)
        rewards_mean = rewards.mean().item()
        rewards_normed = normalize_reward(rewards) if self.args.normalize_reward else rewards
        advantages = compute_advantages(values, rewards_normed, self.prefix_len)

        self.current_metrics['reward'] = rewards_mean

        # store current samples and rewards for potential early stopping save
        self.current_samples = samples
        self.current_rewards = rewards

        # visualization
        n_log_samples = min(5, self.args.batch_size)
        ref_logprobs = get_log_probs(ref_logits[:n_log_samples], sample_ids[:n_log_samples], self.prefix_len).sum(-1)
        headers = ["Reward", "Ref logprobs", "Sample"]
        table_data = [[f"{r:.4f}", f"{lp:.2f}", repr(s)] for r, lp, s in zip(rewards.tolist(), ref_logprobs, samples)]
        table = tabulate(table_data, headers, tablefmt="simple_grid", maxcolwidths=[None, None, 90])
        print(f"Phase {self.phase+1:03}/{self.args.total_phases:03}, Mean reward: {rewards_mean:.4f}\n{table}\n")

        ref_logprobs_mean = ref_logprobs.mean().item()

        return ReplayMemory(
            args = self.args,
            sample_ids = sample_ids,
            logprobs = log_probs,
            advantages = advantages,
            values = values,
            ref_logits = ref_logits)

    def learning_phase(self, memory):
        for minibatch in tqdm(memory.get_minibatches(), desc = f"Learning phase {self.phase+1}"):
            self.optimizer.zero_grad()
            total_obj_fn = self.compute_rlhf_objective(minibatch)
            total_obj_fn.backward()
            # clip according to max_norm
            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = self.args.max_grad_norm)
            self.optimizer.step()

            should_stop = self.early_stopping.should_stop(current_kl = self.current_metrics['kl'],
                                                          current_reward = self.current_metrics['reward'])
            
            if should_stop:
                print(f"Early stopping at phase {self.phase+1}")
                print(f"Step: {self.step}, KL: {self.current_metrics['kl']:.3f}, Reward: {self.current_metrics['reward']:.3f}")
                
                return True

            self.step += 1
            
        self.scheduler.step()
        return False

    def train(self):
        self.step = 0
        self.samples = []

        for self.phase in tqdm(range(self.args.total_phases), desc = "Training phases"):
            memory = self.rollout_phase()
            if self.learning_phase(memory):
                return  # end training if self.learning_phase (early stopping) returns true 
```

## Results

Coming soon... :)