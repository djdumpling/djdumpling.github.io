# Whirlwind of PPO, RLHF and Interp from scratch (in progress)

Summary: we implement RLHF with PPO to fine-tune GPT-2 models for sentiment analysis, then interpret the logits, residual stream, and attention heads. View the full implementation [here](https://github.com/djdumpling/rl/tree/main/rlhf_transformer).

After spending sometime thinking through some basic components of RL like [DQN or PPO on Cartpole](https://github.com/djdumpling/rl/tree/main/dqn_ppo), I became more interested in RLHF, especially as it relates to LLMs and reward hacking. My goal is to help elucidate the training and finetuning process with RLHF and PPO, then describe some results of interpreting the fine-tuned model as it relates to the original model.

## PPO, RLHF, and the Transformer environment

Before any coding is actually done, it's helpful to understand how RLHF and the transformer environment work. 

### PPO
![PPO Algorithm Diagram](/public/IMG_0111.jpg)

*Figure 1: Overview of the PPO algorithm showing the rollout phase (data collection) and training phase (model updates).*

A typical PPO algorithm consists of an agent that interacts with an environment, collects data from the interaction, then updates its weights w.r.t an objective function. Specifically, 

1. The **Actor NN** outputs `num_actions` logits, from which we derive our policy. Based off of that policy, we take an action in the environment.
2. After interacting with the environment, we note the following and store it in memory: 

    a. **Observation**: the current state or input the agent receives from the environment

    b. **Action**: The action taken by the agent

    c. **Log probabilities**: the log probability of the chosen action under the current policy

    d. **Values**: the critic-predicted value of the current state, representing expected future rewards.

    e. **Reward**: the scalar feedback signal received from the environment indicating how good the action was

    f. **Terminated**: a boolean indicating whether the episode has ended
    
3. The experience data is used as calculation in the training phase, particularly to calculate the logits using the Actor NN and the values using the critic NN.

4. Three values are calculated, including the logits and values derived from the experience. 

    a. **Entropy** denotes the amount of uncertainity a distribution has. Given a discrete probability distribution $p$, entropy is given by 

    $$H(p) = \sum_x p(x) \ln\left(\frac1{p(x)}\right)$$

    with the convention that if $p(x) = 0$, then that $0 \ln\left(\frac1{0}\right)=0$. Minimal entropy is achieved when the action is deterministic, $H(p)=0$. Maximal entropy is acheived when $p$ is distributed evenly across the $n$ discrete actions: 
    
    $$H(p)=n \cdot \frac1{n}\ln(n)=\ln(n)$$

    Later, we'll denote this by $S\left[\pi_\theta\right](s_t)$, the entropy of the policy $\pi_\theta$ at state $s_t$. 
    
    b. **Clipped Surrogate Objective** prevents enormous policy updates (hence, **promixal**) of the actor NN by clipping the gradients of the policy. First, define the probability ratio of the current and former policy as $r_t(\theta) = \frac{\pi_\theta(a_t \vert s_t)}{\pi_{old}(a_t \vert s_t)}$ and $\hat{A}_t(s_t, a_t)$ as the **advantage function**, the difference between the expected future reward when taking action $a_t$ in state $s_t$ versus that of taking the expected action according to to the current policy (more on this later). From this, we obtain 

    $$L^{\text{clip}}(\theta)=\frac1{|B|} \sum_t\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t, 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right)\right]$$

    Visually, 
    
    ![clip_ppo](/public/clip_ppo.png)

    *Figure 2: Plots showing the the surrogate function $L^{\text{clip}}$ as a function of the policy ratio $r$. Plots taken from the original PPO paper [here](https://arxiv.org/pdf/1707.06347).*

    If $A > 0$ (i.e. a good action), the function evaluates to a positive (but capped) gradient, leading to more good actions. Conversely, if $A < 0$ and $r_t(\theta) > 1 - \epsilon$, the function evaluates to a negative gradient, leading again to less bad actions. 

    c. **Value Function Loss** helps to optimize the critic NN by minimizing the MSE between the critic's prediction and the observed returns: 

    $$L^{VF}(\theta) = \frac1{|B|} \sum_t \left(V_\theta(s_t) - V_t^{\text{target}}\right)^2 = \frac1{|B|} \sum_t \left(V_\theta(s_t) - \left(V_{\theta_{\text{target}}}(s_t) + \hat{A}_{\theta_{\text{target}}}(s_t, a_t)\right)\right)^2$$

    where $V_\theta(s_t)$ comes via the critic NN and $V_t^{\text{target}}$ is derived from the rollout stage using the sum of the values and advantages. 

5. The three values calculated during step 4 are composed via a linear combination into the final objective where the expectation just denotes the expectation over all batches: 

    $$L_t(\theta) = \hat{\mathbb{E}}\left[L_t^{\text{clip}}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t)\right]$$

    a. Within $L_t^{\text{clip}}(\theta)$, the only hyperparameter is $\epsilon$, normally set to $0.2$. 

    b. $c_1$ controls the emphasis on accurate value estimation; lower $c_1$ values allow the policy to update more aggressively, but the value function updates more slowly, and we risk instability via poor advantage estimation. Conversely, high $c_1$ allow the value function to learn quickly and accurately but will slow down policy learning.

    c. $c_2$ controls the entropy (uncertainity) of the policy, hence controlling the degree of exploration. If $c_2 = 0$ (no entropy regularization), the policy will converge to deterministc actions once it finds a good action, and it risks getting stuck in local optima. This is the typical **exploration vs exploitation trade-off**. 

