# RLHF and Interp from scratch (in progress)

Summary: we implement RLHF with PPO to fine-tune GPT-2 models for sentiment analysis, then interpret the logits, residual stream, and attention heads. View the full implementation [here](https://github.com/djdumpling/rl/tree/main/rlhf_transformer).

After spending sometime thinking through some basic components of RL like [DQN or PPO on Cartpole](https://github.com/djdumpling/rl/tree/main/dqn_ppo), I became more interested in RLHF, especially as it relates to LLMs and reward hacking. My goal is to help elucidate the training and finetuning process with RLHF and PPO, then describe some results of interpreting the fine-tuned model as it relates to the original model.

## PPO, RLHF, and the Transformer environment

Before any coding is actually done, it's helpful to understand how RLHF and the transformer environment work. 

### PPO
![PPO Algorithm Diagram](/public/IMG_0111.jpg)

*Figure 1: Overview of the PPO algorithm showing the rollout phase (data collection) and training phase (model updates).*

A typical PPO algorithm consists of an agent that interacts with an environment, collects data from the interaction, then updates its weights w.r.t an objective function. Specifically, 

1. The **Actor NN** outputs `num_actions` logits, from which we derive our policy. Based off of that policy, we take an action in the environment.
2. After interacting with the environment, we note the following and store it in memory: 

    a. **Observation**: the current state or input the agent receives from the environment

    b. **Action**: The action taken by the agent

    c. **Log probabilities**: the log probability of the chosen action under the current policy

    d. **Values**: the critic-predicted value of the current state, representing expected future rewards.

    e. **Reward**: the scalar feedback signal received from the environment indicating how good the action was

    f. **Terminated**: a boolean indicating whether the episode has ended
    
3. The experience data is used as calculation in the training phase, particularly to calculate the logits using the Actor NN and the values using the critic NN.

4. Three values are calculated, including the logits and values derived from the experience. 

    a. **Entropy** denotes the amount of uncertainity a distribution has. Given a discrete probability distribution $p$, entropy is given by 

    $$H(p) = \sum_x p(x) \ln\left(\frac1{p(x)}\right)$$

    with the convention that if $p(x) = 0$, then that $0 \ln\left(\frac1{0}\right)=0$. Minimal entropy is achieved when the action is deterministic, $H(p)=0$. Maximal entropy is acheived when $p$ is distributed evenly across the $n$ discrete actions: $$H(p)=n \cdot \frac1{n}\ln(n)=\ln(n)$$
    
    b. **Clipped Surrogate Objective** prevents enormous policy updates of the actor NN by clipping the gradients of the policy. First, define the probability ratio of the current and former policy as $r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{old}(a_t | s_t)}$ and $\hat{A}_t$ as the **advantage function**, the difference between the expected future reward when taking action $a_t$ versus that of taking the expected action according to to the current policy (more on this later). From this, we obtain 

    $$L^{\text{clip}}(\theta)=\frac1{|B|} \sum_t\left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t, 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right)\right]$$

Visually, ![clip_ppo](/public/clip_ppo.png)

